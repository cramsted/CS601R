{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For this project my pipelines are as follows:\n",
    "1. SIFT keypoints and descriptors -> vector quantization -> SVM\n",
    "2. SIFT keypoints and descriptors -> LBP -> SVM\n",
    "3. Dense keypoints -> LBP -> Random Forest\n",
    "4. SIFT keypoints and descriptors -> LBP w/ spacial pyramid -> SVM\n",
    "\n",
    "Each of these pipelines will be discussed in detail separately.The rest of the discussion will go as follows:\n",
    "1. Similarity of features that are sorted into the same k means cluster\n",
    "2. The heatmap of the histograms for each category\n",
    "3. Results for detection of birds and butterflies and the associated PR curve\n",
    "\n",
    "**Note:** All code used to generate the shown figures will be added at the end of the notebook, with comments specifying which file the code was copied from. This is due to problems with the ```multiprocessing``` library not working correctly when imported into the notebook. \n",
    "\n",
    "**Note2:** Occasionally when I would run a cell that made use of the ```multiprocessing``` library, it would use all my memory and crash my computer. This problem may come from not fully stopping a previously executed cell before beginning with a new one (i.e. if it is displaying a graph, matplotlib is still running, and by extent so is the program.) This is because these programs can take ~5 Gb of RAM when they run. For that reason, I recommend that you don't run the code in the cells and use a terminal instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT keypoints and descriptors -> Vector Quantization -> SVM\n",
    "\n",
    "![](write_up_images/sift_confusion.png)\n",
    "\n",
    "This particular run of the pipeline used the top 1000 SIFT features for each image of the training set (for a total of ~380k features) and sent it through a k=400 k means clustering algorithm. \n",
    "\n",
    "The accuracy of ~78% was one of the best results I was able to produce, however occasionally a run would break 80%. Prior to this, I was using k=200 and getting values in the 71-74% range, whereas k=400 is more like 75-78% on average. However I saw a dramatic drop in accuracy after k=500.\n",
    "\n",
    "I also found that the number of SIFT features had a significant impact. Using all of the features gave me around 66% accuracy, but only using 500 gave me ~70% fairly consistently. I found that between 1000-1200 features marked the sweet spot where my accuracy plateaued. Because the accuracy varied from run to run, it was not possible to narrow down the range any further. \n",
    "\n",
    "Overall this was my best preforming pipeline, and I used the parameters for k and the number of features that worked well for it as a baseline for the rest of my pipelines. I also did not use normalization for any of my histograms because I found that it destroyed the accuracy on the pipelines. And finally, I resized all of the images so that their y axis was 500 px with the ration of pixels preserved on the x axis. I didn't experiment with this parameter very much, but I found this first order implementation to be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT keypoints and descriptors -> LBP -> SVM\n",
    "\n",
    "![](write_up_images/sift_lbp_confusion.png)\n",
    "\n",
    "To state what will become obvious over the next three pipelines is that anything that uses LBP is terrible. By using the same 1000 SIFT features as before, the pipeline will consistently give ~20% accuracy. \n",
    "\n",
    "I have two theories on why LBP preforms so badly. \n",
    "1. Based on the impact that the number of bins had on vector quantization's accuracy, I suspect that the default 8 bit, 256 bin version of LBP isn't granular to pick out the major descriminating features. \n",
    "2. I think LBP may be looking at too small of a window to get a good idea of what the gradient around a feature point really is doing. By sampling pixels that are further away, I suspect that the type of information is going to be more useful.\n",
    "\n",
    "The nice thing is that testing out both of these methods can easily be done by changing a single aspect of how LBP is coded up, but unfortunately I did not have time to implement those changes to see if they have any merit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense keypoints -> LBP -> Random Forest\n",
    "\n",
    "![](write_up_images/dense_confusion.png)\n",
    "\n",
    "As with the previous pipeline, the accuracy is terrible. But it is not as terrible as it could be. \n",
    "\n",
    "Originally I tried this pipeline out using SVM instead of RF and I consistently got ~15% for accuracy. This indicates that the dense keypoints are inferrior to the SIFT keypoints, but even with that deficency, RF is able to compensate. I find that very surprising, but when I did a test run on the first pipeline with RF, I got a dismal 54% accuracy. Weird.\n",
    "\n",
    "For the run above, I sampled every 5th pixel to get the dense representation. Using 3 pixels produced essentially equivilant results, but 10 pixels saw a significant drop down to 13%. It seems like every bit of information counts when using a dense sampling technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT keypoints and descriptors -> LBP w/ spacial pyramid -> SVM\n",
    "\n",
    "![](write_up_images/sift_lbp_spacial.png)\n",
    "\n",
    "The last pipeline made use of a spacial pyramid that broke the image up into four quadrants and concatonated the LBP histograms for each together. Apparently the spacial data added a significant amount missing information to the LBP, which I interperate as further evidence of my theory that the default LBP implementation doesn't gather enough information around it to be useful. \n",
    "\n",
    "I would be curious to know how the spacial data would improve if the image was segmented up into more chunks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of features that are sorted into the same k means cluster\n",
    "\n",
    "![](write_up_images/similar_features.png)\n",
    "\n",
    "To generate these images, I chose four clusters from a k=400 k means that was generated from ~380k features and looked at a 64x64 area around features that were found to be in those clusters. \n",
    "\n",
    "From the images it is hard to tell what the deliniating feature for each group is. On first look it looks like the selected features are never being fully sorted for some of the clusters. But if you ignore the brightness of the image and instead look at the gradients, it becomes more feasible to find similarities between the images.  \n",
    "\n",
    "Even with that consideration, some of the images (like the one in the lower left of the fourth group) do not seem to share any discernable features with their cluster mates.\n",
    "\n",
    "It could be that I'm looking at bad clusters for this, but I don't think that there is enough data just from the vector quantization to make truly distinct clusters. Adding in some other features like spacial or color information may help with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap of the histograms for each category\n",
    "\n",
    "![](write_up_images/heatmap.png)\n",
    "\n",
    "I believe that there is something wrong with the averaged graph on the right. It is pretty easy to chose a bright line in the right graph and to not be able to find a collection of bright dots in the left graph. Unfortunately I'm not sure what the problem is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for detection of birds and butterflies and the associated PR curve\n",
    "\n",
    "![](write_up_images/detection.png)\n",
    "\n",
    "As expected the detection rate is significantly higher than the classification rates. The PR curve more-or-less matches what the ```sklearn``` library produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT keypoints and descriptors -> vector quantization -> SVM\n",
    "# code taken from sift.py\n",
    "from DataSet import DataSet\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import image_operations as img_op\n",
    "\n",
    "ds = DataSet()\n",
    "data_train = ds.butterfly_train\n",
    "data_test = ds.butterfly_test\n",
    "\n",
    "mbk = MiniBatchKMeans(n_clusters=400)\n",
    "\n",
    "\n",
    "def vector_quantization_train(features):\n",
    "    sift_features = np.empty((0, 128))\n",
    "    for f, label in features:\n",
    "        mbk.partial_fit(f)\n",
    "    return vector_quantization(features)\n",
    "\n",
    "\n",
    "def vector_quantization(features):\n",
    "    X = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    for f, label in features:\n",
    "        count += len(f)\n",
    "        vq = mbk.predict(f)\n",
    "        vals, bins, _ = plt.hist(vq, bins=400, histtype='step')\n",
    "        X.append(vals)\n",
    "        y.append(label)\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    return X, y, count\n",
    "\n",
    "\n",
    "def get_features(args):\n",
    "    i = args[0]\n",
    "    data = args[1]\n",
    "    img = ds.get_image(data[i][1])\n",
    "    label = data[i][0]\n",
    "    sift = cv2.xfeatures2d.SIFT_create(1000)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    return [des, label]\n",
    "\n",
    "\n",
    "pool = Pool(os.cpu_count())\n",
    "print(\"Training\")\n",
    "features = pool.map(get_features, [(i, data_train)\n",
    "                                   for i in range(len(data_train))])\n",
    "\n",
    "X, y, count = vector_quantization_train(features)\n",
    "print(\"Number of features: \", count)\n",
    "clf = LinearSVC()\n",
    "# clf = RandomForestClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Testing\")\n",
    "features = pool.map(get_features, [(i, data_test)\n",
    "                                   for i in range(len(data_test))])\n",
    "X, y, count = vector_quantization(features)\n",
    "print(\"Number of features: \", count)\n",
    "predictions = clf.predict(X)\n",
    "accuracy = np.count_nonzero(np.where(predictions == y)[\n",
    "                            0]) / predictions.shape[0]\n",
    "print(\"Accuracy: \", accuracy)\n",
    "cm = confusion_matrix(y, predictions)\n",
    "plt.figure()\n",
    "img_op.plot_confusion_matrix(\n",
    "    cm, classes=ds.categories, title='SIFT w/ VQ Confusion Matrix  \\nAccuracy={}'.format(accuracy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SIFT keypoints and descriptors -> LBP -> SVM\n",
    "# code in sift_lbp.py\n",
    "from DataSet import DataSet\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import image_operations as img_op\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from sklearn.svm import LinearSVC\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "ds = DataSet()\n",
    "data_train = ds.butterfly_train\n",
    "data_test = ds.butterfly_test\n",
    "\n",
    "lbp_features = np.empty((0))\n",
    "\n",
    "\n",
    "def make_image_histogram(kps, lbp):\n",
    "    kpx = np.asarray([np.around(kp.pt[0]) for kp in kps], dtype=np.int32)\n",
    "    kpy = np.asarray([np.around(kp.pt[1]) for kp in kps], dtype=np.int32)\n",
    "    vector = lbp[kpy, kpx]\n",
    "    vals, bins, _ = plt.hist(vector, bins=256, histtype='step')\n",
    "    return vals\n",
    "\n",
    "\n",
    "def get_features(args):\n",
    "    i = args[0]\n",
    "    data = args[1]\n",
    "    img = ds.get_image(data[i][1])\n",
    "    label = data[i][0]\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = img_op.LBP(gray)\n",
    "    sift = cv2.xfeatures2d.SIFT_create(1000)\n",
    "    kp, des = sift.detectAndCompute(gray, None)\n",
    "    return [make_image_histogram(kp, lbp), label]\n",
    "\n",
    "\n",
    "def clf_format_data(features):\n",
    "    X = []\n",
    "    y = []\n",
    "    for f, label in features:\n",
    "        X.append(f)\n",
    "        y.append(label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "print(\"Training\")\n",
    "# get_features([0, data_train])\n",
    "pool = Pool(os.cpu_count())\n",
    "features = pool.map(get_features, [(i, data_train)\n",
    "                                   for i in range(len(data_train))])\n",
    "X, y = clf_format_data(features)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Testing\")\n",
    "pool = Pool(os.cpu_count())\n",
    "features = pool.map(get_features, [(i, data_test)\n",
    "                                   for i in range(len(data_test))])\n",
    "X, y = clf_format_data(features)\n",
    "predictions = clf.predict(X)\n",
    "accuracy = np.count_nonzero(np.where(predictions == y)[\n",
    "                            0]) / predictions.shape[0]\n",
    "print(\"Accuracy: \", accuracy)\n",
    "cm = confusion_matrix(y, predictions)\n",
    "plt.figure()\n",
    "img_op.plot_confusion_matrix(\n",
    "    cm, classes=ds.categories, title='SIFT w/ LBP Confusion Matrix  \\nAccuracy={}'.format(accuracy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense keypoints -> LBP -> Random Forest\n",
    "# code in dense.py\n",
    "from DataSet import DataSet\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import image_operations as img_op\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import random\n",
    "\n",
    "ds = DataSet()\n",
    "data_train = ds.butterfly_train\n",
    "data_test = ds.butterfly_test\n",
    "\n",
    "lbp_features = np.empty((0))\n",
    "\n",
    "\n",
    "def make_image_histogram(kps, lbp):\n",
    "    kpx = np.asarray([np.around(kp.pt[0]) for kp in kps], dtype=np.int32)\n",
    "    kpy = np.asarray([np.around(kp.pt[1]) for kp in kps], dtype=np.int32)\n",
    "    vector = lbp[kpy, kpx]\n",
    "    vals, bins, _ = plt.hist(vector, bins=256, histtype='step')\n",
    "    return vals\n",
    "\n",
    "\n",
    "def get_features(args):\n",
    "    i = args[0]\n",
    "    data = args[1]\n",
    "    img = ds.get_image(data[i][1])\n",
    "    label = data[i][0]\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = img_op.LBP(gray)\n",
    "    # sampling done by lbp[::x, :;x] where x is number of pixels to skip between samplings\n",
    "    vals, bins, _ = plt.hist(\n",
    "        lbp[::5, ::5].flatten(), bins=256, histtype='step')\n",
    "    return [vals, label]\n",
    "\n",
    "\n",
    "def clf_format_data(features):\n",
    "    X = []\n",
    "    y = []\n",
    "    for f, label in features:\n",
    "        X.append(f)\n",
    "        y.append(label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "print(\"Training\")\n",
    "# get_features([0, data_train])\n",
    "pool = Pool(os.cpu_count())\n",
    "features = pool.map(get_features, [(i, data_train)\n",
    "                                   for i in range(len(data_train))])\n",
    "X, y = clf_format_data(features)\n",
    "clf = LinearSVC()\n",
    "# clf = RandomForestClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Testing\")\n",
    "pool = Pool(os.cpu_count())\n",
    "features = pool.map(get_features, [(i, data_test)\n",
    "                                   for i in range(len(data_test))])\n",
    "X, y = clf_format_data(features)\n",
    "predictions = clf.predict(X)\n",
    "accuracy = np.count_nonzero(np.where(predictions == y)[\n",
    "    0]) / predictions.shape[0]\n",
    "print(\"Accuracy: \", accuracy)\n",
    "cm = confusion_matrix(y, predictions)\n",
    "plt.figure()\n",
    "img_op.plot_confusion_matrix(\n",
    "    cm, classes=ds.categories, title='Dense Sampling w/ LBP Confusion Matrix \\nAccuracy={}'.format(accuracy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT keypoints and descriptors -> LBP w/ spacial pyramid -> SVM\n",
    "# code from sift_lbp_spacial.py\n",
    "from DataSet import DataSet\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import image_operations as img_op\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from sklearn.svm import LinearSVC\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "ds = DataSet()\n",
    "data_train = ds.butterfly_train\n",
    "data_test = ds.butterfly_test\n",
    "\n",
    "lbp_features = np.empty((0))\n",
    "\n",
    "\n",
    "def spacial(lbp, kpy, kpx):\n",
    "    xbound = int(lbp.shape[1]/2)\n",
    "    ybound = int(lbp.shape[0]/2)\n",
    "    quadrants = [[0, xbound, 0, ybound],\n",
    "                 [xbound, lbp.shape[1]+1, 0, ybound],\n",
    "                 [0, xbound, ybound, lbp.shape[0]+1],\n",
    "                 [xbound, lbp.shape[1]+1, ybound, lbp.shape[0]+1]]\n",
    "    vector = np.empty((0))\n",
    "    for xlower, xupper, ylower, yupper in quadrants:\n",
    "        mask = np.logical_and(np.logical_and(np.logical_and(\n",
    "            xlower <= kpx, ylower <= kpy), kpx <= xupper), kpy <= yupper)\n",
    "        vals, bins, _ = plt.hist(\n",
    "            lbp[kpy[mask], kpx[mask]], bins=256, histtype='step')\n",
    "        vector = np.hstack((vector, vals))\n",
    "    return vector\n",
    "\n",
    "\n",
    "def make_image_histogram(kps, lbp):\n",
    "    kpx = np.asarray([np.around(kp.pt[0]) for kp in kps], dtype=np.int32)\n",
    "    kpy = np.asarray([np.around(kp.pt[1]) for kp in kps], dtype=np.int32)\n",
    "    return spacial(lbp, kpy, kpx)\n",
    "\n",
    "\n",
    "def get_features(args):\n",
    "    i = args[0]\n",
    "    data = args[1]\n",
    "    img = ds.get_image(data[i][1])\n",
    "    label = data[i][0]\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = img_op.LBP(gray)\n",
    "    sift = cv2.xfeatures2d.SIFT_create(500)\n",
    "    kp, des = sift.detectAndCompute(gray, None)\n",
    "    return [make_image_histogram(kp, lbp), label]\n",
    "\n",
    "\n",
    "def clf_format_data(features):\n",
    "    X = []\n",
    "    y = []\n",
    "    for f, label in features:\n",
    "        X.append(f)\n",
    "        y.append(label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "print(\"Training\")\n",
    "# get_features([0, data_train])\n",
    "pool = Pool(os.cpu_count())\n",
    "features = pool.map(get_features, [(i, data_train)\n",
    "                                   for i in range(len(data_train))])\n",
    "X, y = clf_format_data(features)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Testing\")\n",
    "pool = Pool(os.cpu_count())\n",
    "features = pool.map(get_features, [(i, data_test)\n",
    "                                   for i in range(len(data_test))])\n",
    "X, y = clf_format_data(features)\n",
    "predictions = clf.predict(X)\n",
    "accuracy = np.count_nonzero(np.where(predictions == y)[\n",
    "                            0]) / predictions.shape[0]\n",
    "print(\"Accuracy: \", accuracy)\n",
    "cm = confusion_matrix(y, predictions)\n",
    "plt.figure()\n",
    "img_op.plot_confusion_matrix(\n",
    "    cm, classes=ds.categories, \\\n",
    "    title='SIFT LBP & Spacial Pyramid pooling Confusion Matrix  \\nAccuracy={}'.format(accuracy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similarity of features that are sorted into the same k means cluster\n",
    "# code in similar_features.py\n",
    "from DataSet import DataSet\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "import image_operations as img_op\n",
    "\n",
    "ds = DataSet()\n",
    "data_train = ds.butterfly_train\n",
    "data_test = ds.butterfly_test\n",
    "\n",
    "mbk = MiniBatchKMeans(n_clusters=400)\n",
    "\n",
    "\n",
    "def get_features(args):\n",
    "    i = args[0]\n",
    "    data = args[1]\n",
    "    img = ds.get_image(data[i][1])\n",
    "    label = data[i][0]\n",
    "    sift = cv2.xfeatures2d.SIFT_create(1000)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    return [kp, des]\n",
    "\n",
    "\n",
    "print(\"Training\")\n",
    "features = []\n",
    "for i in range(50):\n",
    "    features.append(get_features([i, data_train]))\n",
    "\n",
    "sift_features = np.empty((0))\n",
    "for kp, des in features:\n",
    "    mbk.partial_fit(des)\n",
    "\n",
    "similar_patches = []\n",
    "for i in np.random.randint(99, size=6):\n",
    "    patches = []\n",
    "    for j in range(5):\n",
    "        kps, dess = get_features([i, data_train])\n",
    "        for kp, des in zip(kps, dess):\n",
    "            predict = mbk.predict([des])\n",
    "            if predict == j:\n",
    "                patches.append(img_op.getPatchFor(\n",
    "                    kp, cv2.cvtColor(ds.get_image(data_train[i][1]), cv2.COLOR_BGR2GRAY)))\n",
    "    similar_patches.append(patches)\n",
    "images = []\n",
    "for patches in similar_patches:\n",
    "    if len(patches) >= 6:\n",
    "        top_row = np.hstack((patches[0], patches[1]))\n",
    "        top_row = np.hstack((top_row, patches[2]))\n",
    "        # top_row = np.hstack((top_row, patches[3]))\n",
    "        bottom_row = np.hstack((patches[3], patches[4]))\n",
    "        bottom_row = np.hstack((bottom_row, patches[5]))\n",
    "        # bottom_row = np.hstack((bottom_row, patches[7]))\n",
    "        images.append(np.vstack((top_row, bottom_row)))\n",
    "print(len(images))\n",
    "for i in range(1, 5):\n",
    "    plt.subplot(4, 1, i)\n",
    "    plt.imshow(cv2.cvtColor(images[i-1], cv2.COLOR_GRAY2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The heatmap of the histograms for each category\n",
    "# coded in k_dim_histogram.py\n",
    "from DataSet import DataSet\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "import image_operations as img_op\n",
    "\n",
    "ds = DataSet()\n",
    "data = ds.k_histogram\n",
    "\n",
    "mbk = MiniBatchKMeans(n_clusters=200)\n",
    "\n",
    "\n",
    "def vector_quantization_train(features):\n",
    "    sift_features = np.empty((0, 128))\n",
    "    for f, label in features:\n",
    "        mbk.partial_fit(f)\n",
    "    return vector_quantization(features)\n",
    "\n",
    "\n",
    "def vector_quantization(features):\n",
    "    X = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    for f, label in features:\n",
    "        count += len(f)\n",
    "        vq = mbk.predict(f)\n",
    "        vals, bins, _ = plt.hist(vq, bins=200, histtype='step')\n",
    "        X.append(vals)\n",
    "        y.append(label)\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    return X, y, count\n",
    "\n",
    "\n",
    "def get_features(args):\n",
    "    i = args[0]\n",
    "    data = args[1]\n",
    "    img = ds.get_image(data[i][1])\n",
    "    label = data[i][0]\n",
    "    sift = cv2.xfeatures2d.SIFT_create(1000)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    return [des, label]\n",
    "\n",
    "\n",
    "pool = Pool(os.cpu_count())\n",
    "print(\"Training\")\n",
    "features = pool.map(get_features, [(i, data)\n",
    "                                   for i in range(len(data))])\n",
    "\n",
    "X, y, count = vector_quantization_train(features)\n",
    "X = np.asarray(X).T\n",
    "y = np.asarray(y)\n",
    "plt.subplot(121)\n",
    "plt.imshow(X, cmap='jet', interpolation='nearest')\n",
    "plt.title(\"Heat Map of All Histograms\")\n",
    "\n",
    "# X_category = np.ones(X.shape)\n",
    "# y = np.asarray(y)\n",
    "# for i in range(1, 11):\n",
    "#     mask = y == i\n",
    "#     avg = np.mean(X[:, mask], axis=0)\n",
    "#     X_category[:, mask] = X_category[:, mask] * avg[:, np.newaxis]\n",
    "for i in range(1, 11):\n",
    "    mask = y == i\n",
    "    avg = np.around(np.mean(X[:, mask], axis=1))\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    for j in range(21):\n",
    "        if i == 1:\n",
    "            X_cat = avg\n",
    "        else:\n",
    "            X_cat = np.vstack((X_cat, avg))\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_cat.T, cmap='jet', interpolation='nearest')\n",
    "plt.title(\"Heat Map of Averaged Histograms for each Catgory\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Results for detection of birds and butterflies and the associated PR curve\n",
    "# code in detection.py\n",
    "from DataSet import DataSet\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle\n",
    "import image_operations as img_op\n",
    "\n",
    "ds = DataSet()\n",
    "data_train = ds.training_set\n",
    "data_test = ds.test_set\n",
    "\n",
    "mbk = MiniBatchKMeans(n_clusters=400)\n",
    "\n",
    "\n",
    "def vector_quantization_train(features):\n",
    "    sift_features = np.empty((0, 128))\n",
    "    for f, label in features:\n",
    "        mbk.partial_fit(f)\n",
    "    return vector_quantization(features)\n",
    "\n",
    "\n",
    "def vector_quantization(features):\n",
    "    X = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    for f, label in features:\n",
    "        count += len(f)\n",
    "        vq = mbk.predict(f)\n",
    "        vals, bins, _ = plt.hist(vq, bins=400, histtype='step')\n",
    "        X.append(vals)\n",
    "        y.append(label)\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    return X, y, count\n",
    "\n",
    "\n",
    "def get_features(args):\n",
    "    i = args[0]\n",
    "    data = args[1]\n",
    "    img = ds.get_image(data[i][1])\n",
    "    label = data[i][0]\n",
    "    sift = cv2.xfeatures2d.SIFT_create(500)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    return [des, label]\n",
    "\n",
    "\n",
    "pool = Pool(os.cpu_count())\n",
    "print(\"Training\")\n",
    "features = pool.map(get_features, [(i, data_train)\n",
    "                                   for i in range(len(data_train))])\n",
    "\n",
    "X, y, count = vector_quantization_train(features)\n",
    "print(\"Number of features: \", count)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Testing\")\n",
    "features = pool.map(get_features, [(i, data_test)\n",
    "                                   for i in range(len(data_test))])\n",
    "X, y, count = vector_quantization(features)\n",
    "print(\"Number of features: \", count)\n",
    "predictions = clf.predict(X)\n",
    "accuracy = np.count_nonzero(np.where(predictions == y)[\n",
    "                            0]) / predictions.shape[0]\n",
    "print(\"Accuracy \", accuracy)\n",
    "cm = confusion_matrix(y, predictions)\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "img_op.plot_confusion_matrix(\n",
    "    cm, classes=['butterflies', 'birds'], title='SIFT w/ VQ Confusion Matrix \\nAccuracy={}'.format(accuracy))\n",
    "plt.subplot(122)\n",
    "confidences = clf.decision_function(X)\n",
    "img_op.precision_recall(y, confidences)\n",
    "# img_op.precision_recall(y, predictions)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
